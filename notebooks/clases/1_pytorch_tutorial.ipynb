{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de PyTorch\n",
    "\n",
    "La forma más recomendada es usando el manejador de ambientes y paquetes `conda`\n",
    "\n",
    "Si no conoces conda por favor revisa esta breve clase de INFO147: https://github.com/magister-informatica-uach/INFO147/blob/master/unidad1/02_ambientes_virtuales.ipynb\n",
    "\n",
    "Una vez hayas creado un ambiente de `conda` debes escoger entre instalar pytorch con soporte de GPU\n",
    "\n",
    "    conda install pytorch torchvision cudatoolkit=10.2 ignite -c pytorch\n",
    "\n",
    "o sin soporte GPU \n",
    "\n",
    "    conda install pytorch torchvision cpuonly ignite -c pytorch\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "tensor_illustration.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breve tutorial de [PyTorch](https://pytorch.org) \n",
    "\n",
    "PyTorch es una librería de alto nivel para Python que provee \n",
    "1. Una clase tensor para hacer cómputo de alto rendimiento \n",
    "1. Un plataforma para crear y entrenar redes neuronales\n",
    "\n",
    "### Torch Tensor\n",
    "\n",
    "La clase [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) es muy similar en uso al `ndarray` de [*NumPy*](https://numpy.org/)\n",
    "\n",
    "Un tensor corresponde a una matriz o arreglo n-dimensional con tipo definido que soporta operaciónes vectoriales tipo SIMD y broadcasting\n",
    "\n",
    "\n",
    "![tensor_illustration.png](attachment:tensor_illustration.png)\n",
    "\n",
    "La documentación de la clase con todas las operaciones que soporta: https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "A continuación revisaremos las más fundamentales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de tensores\n",
    "\n",
    "Un tensor puede crearse usando constructores de torch o a partir de datos existentes: lista de Python o *ndarray* de NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un tensor de 10 ceros\n",
    "display(torch.zeros(10))\n",
    "# Un tensor de 10 unos\n",
    "display(torch.ones(10))\n",
    "# Un tensor de números linealmente espaciados\n",
    "display(torch.linspace(0, 9, steps=10))\n",
    "# Un tensor de 10 números aleatorios con distribución N(0, 1)\n",
    "display(torch.randn(10))\n",
    "# Un tensor creado a partir de una lista\n",
    "display(torch.Tensor([0, 1, 2, 3, 4, 5, 6]))\n",
    "# Un tensor creado a partir de un ndarray\n",
    "numpy_array = np.random.randn(10)\n",
    "display(torch.from_numpy(numpy_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atributos importantes de los tensores\n",
    "\n",
    "Un tensor tiene un tamaño (dimesiones) y tipo específico\n",
    "\n",
    "Un tensor puede estar alojado en la memoria del sistema ('cpu') o en la memoria de dispositivo ('gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10, 20, 30)\n",
    "display(a.shape)\n",
    "display(a.dtype)\n",
    "display(a.device)\n",
    "display(a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se crea un tensor se puede especificar el tipo y el dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(10, dtype=torch.int32, device='cuda')\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulación de tensores\n",
    "\n",
    "Podemos manipular la forma de un tensor usando: reshape, flatten, roll, traspose, unsqueeze, entre otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0, 9, 10)\n",
    "display(a)\n",
    "display(a.reshape(2, 5))\n",
    "display(a.reshape(2, 5).t())\n",
    "display(a.reshape(2, 5).flatten())\n",
    "display(a.roll(2))\n",
    "display(a.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculos con tensores\n",
    "\n",
    "Un tensor soporta operaciones aritméticas y lógicas\n",
    "\n",
    "Si el tensor está en memoria de sistema entonces las operaciones son realizadas por la CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0, 5, steps=6)\n",
    "display(a)\n",
    "# aritmética y funciones\n",
    "display(a + 5)\n",
    "display(2*a)\n",
    "display(a.pow(2))\n",
    "display(a.log())\n",
    "# máscaras booleanas\n",
    "display(a[a>3])\n",
    "# Operaciones con otros tensores\n",
    "b = torch.ones(6)\n",
    "display(a + b)\n",
    "display(a*b)\n",
    "# broadcasting\n",
    "display(a.unsqueeze(1)*b.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculos en GPU\n",
    "\n",
    "Usando el atributo `to` podemos intercambiar un tensor entre GPU ('device') y CPU ('host')\n",
    "\n",
    "Cuando todos los tensores involucrados en una operaciones están en memoria de dispositivo entonces el cálculo lo hace la GPU\n",
    "\n",
    "La siguiente nota indica las opciones para intercambiar datos entre GPU y CPU que ofrece PyTorch: https://pytorch.org/docs/stable/notes/cuda.html \n",
    "\n",
    "##### Breve nota: \n",
    "Una *Graphical Processing Unit* (GPU) o tarjeta de video es un hardware para hacer cálculos sobre mallas tridimensionales, generación de imágenes (rendering) y otras tareas gráficas. A diferencia de la CPU, la GPU es especialista en cálculo paralelo y tiene miles de nucleos (NVIDIA RTX 2080: 2944 nucleos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(10)\n",
    "display(a.device)\n",
    "a = a.to('cuda')\n",
    "display(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-diferenciación\n",
    "\n",
    "Las redes neuronales se entrenan usando **Gradiente descedente**\n",
    "\n",
    "> Necesitamos calcular las derivadas de la función de costo para todos los parámetros de la red\n",
    "\n",
    "Esto puede ser complejo si nuestra red es grande y tiene distintos tipos de capas\n",
    "\n",
    "PyTorch viene incorporado con un sistema de diferenciación automática denominado [`autograd`](https://pytorch.org/docs/stable/autograd.html) \n",
    "\n",
    "Para poder derivar una función en pytorch\n",
    "\n",
    "1. Se necesita que su entrada sean tensores con el atributo `requires_grad=True`\n",
    "1. Luego llamamos la función `backward()` de la función\n",
    "1. El resultado queda guardado en el atributo `grad` de la entrada (nodo hoja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 10, steps=1000, requires_grad=True)\n",
    "\n",
    "y = 5*x -20\n",
    "#y = torch.sin(2.0*np.pi*x)*torch.exp(-(x-5).pow(2)/3)\n",
    "#dydx = 2*np.pi*torch.cos(2.0*np.pi*x)*torch.exp(-(x-5).pow(2)/3) - 2/3*(x-5)*torch.sin(2.0*np.pi*x)*torch.exp(-(x-5).pow(2)/3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(x.detach().numpy(), y.detach().numpy(), label='y')\n",
    "\n",
    "y.backward(torch.ones_like(x))\n",
    "\n",
    "ax.plot(x.detach().numpy(), x.grad.detach().numpy(), label='dy/dx')\n",
    "#ax.plot(x.detach().numpy(), dydx.detach().numpy())\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo de cómputo\n",
    "\n",
    "Cuando contatenamos operaciones PyTorch construye internamente un \"grafo de cómputo\"\n",
    "\n",
    "$$\n",
    "x \\to z = f_1(x) \\to y = f_2(z)\n",
    "$$\n",
    "\n",
    "La función `backward` calcula los gradientes y los almacena en los nodo hoja que tengan `requires_grad=True`\n",
    "\n",
    "Por ejemplo\n",
    "\n",
    "    y.backward : Guarda dy/dx en x.grad\n",
    "    \n",
    "    z.backward : Guarda dz/dx en x.grad\n",
    "\n",
    "Basicamente `backward` implementa la regla de la cadena de las derivadas\n",
    "\n",
    "`backward` recibe una entrada: La derivada de la etapa superior de la cadena. Por defecto usa `torch.ones([1])`, asume que se está en el nivel superior y que la salida es escalar (unidimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 10, steps=1000, requires_grad=True) # Nodo hoja\n",
    "display(x.grad_fn)\n",
    "z = torch.sin(2*x)\n",
    "display(z.grad_fn)\n",
    "y = z.pow(2)/2\n",
    "display(y.grad_fn)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.plot(x.detach().numpy(), z.detach().numpy(), label='z')\n",
    "ax.plot(x.detach().numpy(), y.detach().numpy(), label='y')\n",
    "\n",
    "# Derivada dy/dx\n",
    "y.backward(torch.ones_like(x), create_graph=True)\n",
    "ax.plot(x.detach().numpy(), x.grad.detach().numpy(), label='dy/dx')\n",
    "\n",
    "# Borro el resultado en x.grad\n",
    "x.grad = None\n",
    "\n",
    "# Derivada dz/dx\n",
    "z.backward(torch.ones_like(x))\n",
    "ax.plot(x.detach().numpy(), x.grad.detach().numpy(), label='dz/dx')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Definir una red neuronal en PyTorch\n",
    "\n",
    "PyTorch nos ofrece la clase tensor y las funcionalidades de autograd\n",
    "\n",
    "Estas poderosas herramientas nos dan todo lo necesario para construir y entrenar redes neuronales artificiales\n",
    "\n",
    "Para facilitar aun más estas tareas PyTorch tiene módulos de alto nivel que implementan\n",
    "\n",
    "1. Modelo base de red neuronal: `torch.nn.Module`\n",
    "1. Distintos tipos de capas, funciones de activación y funciones de costo: [`torch.nn`](https://pytorch.org/docs/stable/nn.html)\n",
    "1. Distintos algoritmos de optimización basados en gradiente descedente: [`torch.optim`](https://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "\n",
    "Una red neuronal en PyTorch se implementa\n",
    "1. Heredando de [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#module)\n",
    "1. Especificando las funciones `__init__` y `forward`\n",
    "\n",
    "Otra opción es usar [`torch.nn.Sequential`](https://pytorch.org/docs/stable/nn.html#sequential) y especificar una lista de capas\n",
    "\n",
    "\n",
    "#### Red MLP en pytorch:\n",
    "\n",
    "Heredamos de `Module` y especificamos el constructor y la función `forward`\n",
    "\n",
    "Creamos una red de dos entradas, una capa oculta y una neurona salida\n",
    "\n",
    "La capa `torch.nn.Linear` con parámetro $W$ y $B$ realiza la siguiente operación sobre la entrada $X$\n",
    "\n",
    "$$\n",
    "Z = WX + B\n",
    "$$\n",
    "\n",
    "corresponde a la capa completamente conectada (*fully-connected*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    # Constructor: Crea las capas de la red\n",
    "    def __init__(self): \n",
    "        # Mandatorio: Llamar al constructor del padre:\n",
    "        super(type(self), self).__init__()  \n",
    "        # Creamos dos capas completamente conectadas\n",
    "        ?\n",
    "        ?\n",
    "        # Función de activación sigmoide\n",
    "        ?\n",
    "        \n",
    "    # Forward: Conecta la entrada con la salida\n",
    "    def forward(self, x):\n",
    "        # Pasamos x por la primera capa y luego aplicamos función de activación\n",
    "        ?\n",
    "        # Pasamos el resultado por la segunda capa y lo retornamos\n",
    "        return ?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al crear una capa `Linear` de forma interna se registran los parámetros `weight` y `bias`, ambos con `requires_grad=True`\n",
    "\n",
    "Inicialmente los parámetros tienen valores aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_dim=2)\n",
    "display(model.hidden.weight)\n",
    "display(model.hidden.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar el modelo sobre un tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 10*torch.rand(10000, 2) - 5\n",
    "Y = model.forward(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "X_numpy = X.detach().numpy()\n",
    "Y_numpy = Y.detach().numpy()\n",
    "ax.scatter(X_numpy[:, 0], X_numpy[:, 1], s=10, c=Y_numpy[:, 0], cmap=plt.cm.RdBu_r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de redes neuronales en Pytorch\n",
    "\n",
    "Para entrenar la neurona debemos definir \n",
    "\n",
    "1. Una función de costo: La \"qué\" vamos a minimizar\n",
    "1. Un algoritmo de optimización: El \"cómo\" vamos a minimizar\n",
    "\n",
    "Las funciones de costo están implementadas [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "y tienen la siguiente sintaxis\n",
    "```python\n",
    ">>> # Definimos el criterio\n",
    ">>> criterion = torch.nn.CrossEntropyLoss(reduction, # String, opciones 'sum', 'mean' o 'none'\n",
    "                                          weight=None, # Ponderación opcional para las clases\n",
    "                                          ...\n",
    "                                         )\n",
    ">>> # Lo usamos para calcular la calidad de la predicción\n",
    ">>> loss = criterion(output,  # Minibatch de datos\n",
    "                     target # Minibatch de etiquetas\n",
    "                    ) \n",
    "```\n",
    "\n",
    "Las funciones de costo más comunes son\n",
    "\n",
    "| Problema | Recibe | Etiqueta | Función de costo | \n",
    "|----| ----| ---- | ---- |\n",
    "|Regresión | Real | Real | `torch.nn.MSELoss()` | \n",
    "|Clasificación | [0, 1] | {0, 1} | `torch.nn.BCELoss()` | \n",
    "|Clasificación | Real | {0, 1} | `torch.nn.BCEWithLogitsLoss()` |\n",
    "|Clasificación (multiclase) | LogSoftmax | Entero |`torch.nn.LogLoss()` |\n",
    "|Clasificación (multiclase) | Real | Entero |`torch.nn.CrossEntropyLoss()` |\n",
    "\n",
    "\n",
    "> También puedes escribir tu mismo una función de costo usando aritmética y funciones como `torch.sum` o `torch.mean`\n",
    "\n",
    "Los algoritmos de optimización están implementados en el módulo [`torch.optim`](https://pytorch.org/docs/stable/optim.html?highlight=optim#module-torch.optim)\n",
    "\n",
    "La sintaxis general es\n",
    "\n",
    "```python\n",
    ">>> # Definimos un modelo de torch\n",
    ">>> model = ClaseTorchModule(...) \n",
    ">>> # Creamos el optimizador y le entregamos los parámetros que queremos que actualice\n",
    ">>> optimizer = torch.optim.SGD(model.parameters(), # Los parámetros del modelo\n",
    "                                lr=0.01, # La tasa de aprendizaje\n",
    "                                weight_decay=0, # Penalización L2 de los parámetros (lambda)\n",
    "                                momentum=0.9, # Otros parámetros específicos de cada optimizer\n",
    "                                ...\n",
    "                               )\n",
    "```\n",
    "Los optimizadores más comunes son\n",
    "\n",
    "| Optimizador | Descripción |\n",
    "| ---- | ---- |\n",
    "| `torch.optim.SGD` | Gradiente descedente estocástico con momentum|\n",
    "| `torch.optim.Adam` | Gradiente descedente con tasa de aprendizaje y momentum adaptivo |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de costo entropía cruzada binaria\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "# Algoritmo de optimización Gradiente Descendente Estocástico\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esquema general de entrenamiento\n",
    "\n",
    "\n",
    "\n",
    "``` python\n",
    ">>> for epoch in range(num_epochs): # Durante un cierto número de épocas\n",
    "        for minibatch in data: # Para cada minibatch de datos\n",
    "            optimizer.zero_grad() # Limpiamos los gradientes\n",
    "            x, y = minibatch # Desempaquetamos\n",
    "            yhat = model.forward(x) # Predecimos\n",
    "            loss = criterion(yhat, y) # Evaluamos\n",
    "            loss.backward() # Calculamos los gradientes\n",
    "            optimizer.step() # Actualizamos los parámetros\n",
    "```\n",
    "\n",
    "- Una época es una presentación completa del conjunto de entrenamiento\n",
    "- Un minibatch es un subconjunto del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando con un solo dato\n",
    "\n",
    "Digamos que tenemos un dato $X$ y una etiqueta $Y$\n",
    "\n",
    "> Calculemos el error y actualizemos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[-1.0, 1.0]])\n",
    "Y = torch.tensor([[0.]])\n",
    "\n",
    "hatY = model.forward(X)\n",
    "display(hatY)\n",
    "loss = criterion(hatY, Y)\n",
    "display(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que calculamos la loss podemos calcular el gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "display(model.output.weight.grad)\n",
    "display(model.output.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente actualizamos los parámetros usando la función `step` de nuestro optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetea los gradientes\n",
    "display(model.output.weight)\n",
    "display(model.output.bias)\n",
    "optimizer.step()\n",
    "display(model.output.weight)\n",
    "display(model.output.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos este proceso a través de varias \"épocas\" de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nepoch in range(10):\n",
    "    # Calculamos la salida del modelo\n",
    "    hatY = model.forward(X)\n",
    "    # Reseteamos los gradientes de la iteración anterior\n",
    "    optimizer.zero_grad()\n",
    "    # Calculamos la función de costo\n",
    "    loss = criterion(hatY, Y)\n",
    "    # Calculamos su gradiente\n",
    "    loss.backward()\n",
    "    # Actualizamos los parámetros\n",
    "    optimizer.step()\n",
    "    print(\"%d w:%f %f b:%f\" %(nepoch, model.output.weight[0, 0], model.output.weight[0, 1], model.output.bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando en un conjunto de datos\n",
    "\n",
    "Consideremos un conjunto de entrenamiento con datos bidimensionales y dos clases como el siguiente\n",
    "\n",
    "\n",
    "\n",
    "> Notemos que no es linealmente separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data, labels = sklearn.datasets.make_circles(n_samples=1000, noise=0.2, factor=0.25)\n",
    "#data, labels = sklearn.datasets.make_moons(n_samples=1000, noise=0.2)\n",
    "#data, labels = sklearn.datasets.make_blobs(n_samples=[250]*4, n_features=2, cluster_std=0.5,\n",
    "#                                          centers=np.array([[-1, 1], [1, 1], [-1, -1], [1, -1]]))\n",
    "#labels[labels==2] = 1; labels[labels==3] = 0;\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "for k, marker in enumerate(['x', 'o']):\n",
    "    ax.scatter(data[labels==k, 0], data[labels==k, 1], s=20, marker=marker, alpha=0.75)\n",
    "    \n",
    "# Para las gráficas\n",
    "x_min, x_max = data[:, 0].min() - 0.5, data[:, 0].max() + 0.5\n",
    "y_min, y_max = data[:, 1].min() - 0.5, data[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05), np.arange(y_min, y_max, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Antes de empezar el entrenamiento convertimos los datos a formato tensor de PyTorch\n",
    "1. Luego presentamos los datos en *mini-batches* a la red neuronal en cada época del entrenamiento\n",
    "\n",
    "PyTorch provee las clases `DataSet` y `DataLoader` para lograr estos objetivos\n",
    "\n",
    "Estas clases son parte del módulo data: https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "\n",
    "Crearemos un set a partir de tensores usando una clase que hereda de `DataSet`\n",
    "\n",
    "```python\n",
    "    torch.utils.data.TensorDataset(*tensors # Una secuencia de tensores\n",
    "                                  )\n",
    "```    \n",
    "Luego crearemos conjuntos de entrenamiento y validación usando \n",
    "\n",
    "```python\n",
    "    torch.utils.data.Subset(dataset, # Un objeto que herede de DataSet\n",
    "                            indices # Un conjunto de índices para seleccionar un subconjunto de dataset\n",
    "                           )\n",
    "```    \n",
    "Finalmente crearemos dataloaders usando\n",
    "\n",
    "```python\n",
    "    torch.utils.data.DataLoader(dataset, # Un objeto que herede de DataSet\n",
    "                                batch_size=1, # Tamaño del minibatch \n",
    "                                shuffle=False, # Entregar los minibatches desordenados\n",
    "                                sampler=None, # O especificar una muestreador customizado\n",
    "                                num_workers=0, # Cuantos nucleos de CPU usar\n",
    "                                ...\n",
    "                               )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "# Separamos el data set en entrenamiento y validación\n",
    "train_idx, valid_idx = next(sklearn.model_selection.ShuffleSplit(train_size=0.6).split(data, labels))\n",
    "\n",
    "\n",
    "# Crear conjuntos de entrenamiento y prueba\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset \n",
    "\n",
    "# Creamos un conjunto de datos en formato tensor\n",
    "torch_set = TensorDataset(torch.from_numpy(data.astype('float32')), \n",
    "                          torch.from_numpy(labels.astype('float32')))\n",
    "\n",
    "# Data loader de entrenamiento\n",
    "torch_train_loader = DataLoader(Subset(torch_set, train_idx), shuffle=True, batch_size=32)\n",
    "# Data loader de validación\n",
    "torch_valid_loader = DataLoader(Subset(torch_set, valid_idx), shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los `DataLoader` se ocupan como iteradores de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_data, sample_label in torch_train_loader:\n",
    "    display(sample_data.shape)\n",
    "    display(sample_label.shape)\n",
    "    display(sample_label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recordemos\n",
    "\n",
    "Para cada dato de entrenamiento:\n",
    "- Calculamos gradientes: `loss.backward`\n",
    "- Actualizamos parámetros `optimizer.step`\n",
    "\n",
    "Para cada dato de validación\n",
    "- Evaluamos la *loss* para detectar sobre-ajuste\n",
    "\n",
    "\n",
    "\n",
    "#### ¿Cuándo nos detenemos?\n",
    "\n",
    "Lo ideal es detener el entrenamiento cuando la loss de validación no haya disminuido durante una cierta cantidad de épocas\n",
    "\n",
    "Podemos usar [`save`](https://pytorch.org/tutorials/beginner/saving_loading_models.html) para ir guardando los parámetros del mejor modelo de validación\n",
    "\n",
    "Usamos un número fijo de épocas como resguardo: Si el modelo no ha convergido entonces debemos incrementarlo\n",
    "\n",
    "#### ¿Cómo afecta el resultado el número de neuronas en la capa oculta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_dim=3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "n_epochs = 200\n",
    "running_loss = np.zeros(shape=(n_epochs, 2))\n",
    "\n",
    "best_valid = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(k, model, criterion, optimizer):\n",
    "    global best_valid\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    \n",
    "    # Loop de entrenamiento\n",
    "    for sample_data, sample_label in torch_train_loader:\n",
    "        output = model.forward(sample_data)\n",
    "        optimizer.zero_grad()        \n",
    "        loss = criterion(output, sample_label.unsqueeze(1))  \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Loop de validación\n",
    "    for sample_data, sample_label in torch_valid_loader:\n",
    "        output = model.forward(sample_data)\n",
    "        loss = criterion(output, sample_label.unsqueeze(1))  \n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    # Guardar modelo si es el mejor hasta ahora\n",
    "    if k % 10 == 0:\n",
    "        if valid_loss < best_valid:\n",
    "            best_valid = valid_loss\n",
    "            torch.save({'epoca': k,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': valid_loss}, '/home/phuijse/modelos/best_model.pt')\n",
    "    \n",
    "    return train_loss/torch_train_loader.dataset.__len__(), valid_loss/torch_valid_loader.dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(k):\n",
    "    global model, running_loss\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    running_loss[k, 0], running_loss[k, 1] = train_one_epoch(k, model, criterion, optimizer)\n",
    "    Z = model.forward(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32')))\n",
    "    Z = Z.detach().numpy().reshape(xx.shape)\n",
    "    ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=1., vmin=0, vmax=1)\n",
    "    for i, (marker, name) in enumerate(zip(['o', 'x'], ['Train', 'Test'])):\n",
    "        ax[0].scatter(data[labels==i, 0], data[labels==i, 1], color='k', s=10, marker=marker, alpha=0.5)\n",
    "        ax[1].plot(np.arange(0, k+1, step=1), running_loss[:k+1, i], '-', label=name+\" cost\")\n",
    "    plt.legend(); ax[1].grid()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3.5), tight_layout=True)\n",
    "update_plot(0)\n",
    "anim = animation.FuncAnimation(fig, update_plot, frames=n_epochs, \n",
    "                               interval=10, repeat=False, blit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuronas de capa oculta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, model.hidden.out_features, figsize=(8, 3), tight_layout=True)\n",
    "\n",
    "Z = model.hidden(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))).detach().numpy()\n",
    "Z = 1/(1+np.exp(-Z))\n",
    "for i in range(model.hidden.out_features):\n",
    "    ax[i].contourf(xx, yy, Z[:, i].reshape(xx.shape), \n",
    "                   cmap=plt.cm.RdBu_r, alpha=1., vmin=np.amin(Z), vmax=np.amax(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recuperando el mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLayerPerceptron(hidden_dim=3)\n",
    "\n",
    "print(\"state_dict del módelo:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n",
    "\n",
    "\n",
    "    \n",
    "model.load_state_dict(torch.load('/home/phuijse/modelos/best_model.pt')['model_state_dict'])\n",
    "\n",
    "print(\" \")\n",
    "print(\"state_dict del módelo recuperado:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnósticos a partir de curvas de aprendizaje\n",
    "\n",
    "Podemos diagnosticar el entrenamiento observando la evolución de la loss \n",
    "\n",
    "Siempre visualiza la loss en ambos conjuntos: entrenamiento y validación\n",
    "\n",
    "Algunos ejemplos\n",
    "\n",
    "#### Ambas curvas en descenso\n",
    "\n",
    "- Entrena por más épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 500)\n",
    "loss_train = (epochs)**(-1/10) + 0.01*np.random.randn(len(epochs))\n",
    "loss_valid = (epochs)**(-1/10) + 0.01*np.random.randn(len(epochs)) + 0.1\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(epochs, loss_train, lw=2, label='entrenamiento')\n",
    "ax.plot(epochs, loss_valid, lw=2, label='validación')\n",
    "ax.set_ylim([0.5, 1.05])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muy poca diferencia entre error de entrenamiento y validación\n",
    "\n",
    "- Entrena por más épocas\n",
    "- Usa un modelo más complejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 500)\n",
    "loss_train = (epochs)**(-1/10) + 0.01*np.random.randn(len(epochs))\n",
    "loss_valid = (epochs)**(-1/10) + 0.01*np.random.randn(len(epochs)) + 0.01\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(epochs, loss_train, lw=2, label='entrenamiento')\n",
    "ax.plot(epochs, loss_valid, lw=2, label='validación')\n",
    "ax.set_ylim([0.5, 1.05])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sobreajuste temprano\n",
    "\n",
    "- Usa un modelo más sencillo\n",
    "- Usa más datos (aumentación)\n",
    "- Usa regularización (dropout, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 500)\n",
    "loss_train = (epochs)**(-1/10) + 0.01*np.random.randn(len(epochs))\n",
    "loss_valid = (epochs)**(-1/10) + 0.00001*(epochs)**2 +0.01*np.random.randn(len(epochs)) + 0.01\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(epochs, loss_train, lw=2, label='entrenamiento')\n",
    "ax.plot(epochs, loss_valid, lw=2, label='validación')\n",
    "ax.set_ylim([0.5, 1.05])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error en el código o mal punto de partida\n",
    "\n",
    "- Revisa que tu código no tenga bugs\n",
    "    - Función de costo\n",
    "    - Optimizador\n",
    "- Mala inicialización, reinicia el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, 500)\n",
    "loss_train = 1.0 + 0.01*np.random.randn(len(epochs))\n",
    "loss_valid = 1.0 + 0.01*np.random.randn(len(epochs)) + 0.01\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.plot(epochs, loss_train, lw=2, label='entrenamiento')\n",
    "ax.plot(epochs, loss_valid, lw=2, label='validación')\n",
    "#ax.set_ylim([0.5, 1.05])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

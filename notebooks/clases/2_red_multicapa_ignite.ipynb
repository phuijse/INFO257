{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de datos\n",
    "\n",
    "- Datos de ejemplo: Problema no linealmente separable\n",
    "- DataSet y DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#data, labels = sklearn.datasets.make_circles(n_samples=1000, noise=0.2, factor=0.25)\n",
    "data, labels = sklearn.datasets.make_blobs(n_samples=[300]*3, n_features=2, cluster_std=0.5,\n",
    "                                          centers=np.array([[-1, 1], [1, 1], [-1, -1]]))\n",
    "labels[labels==2] = 1\n",
    "\n",
    "n_input = data.shape[1] # Dimensionalidad de la entrada\n",
    "n_classes = len(np.unique(labels)) # Número de clases\n",
    "symbols = ['x', 'o', 'd', '+']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n",
    "for k, marker in enumerate(symbols[:n_classes]):\n",
    "    ax.scatter(data[labels==k, 0], data[labels==k, 1], \n",
    "               c='k', s=20, marker=marker, alpha=0.75)\n",
    "    \n",
    "# Para las gráficas\n",
    "x_min, x_max = data[:, 0].min() - 0.5, data[:, 0].max() + 0.5\n",
    "y_min, y_max = data[:, 1].min() - 0.5, data[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "import sklearn.model_selection\n",
    "# Separamos el data set en entrenamiento y validación\n",
    "train_idx, valid_idx = next(StratifiedShuffleSplit(train_size=0.75).split(data, labels))\n",
    "\n",
    "\n",
    "# Crear conjuntos de entrenamiento y prueba\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset \n",
    "\n",
    "# Creamos un conjunto de datos en formato tensor\n",
    "torch_set = TensorDataset(torch.from_numpy(data.astype('float32')), \n",
    "                          torch.from_numpy(labels))\n",
    "\n",
    "# Data loader de entrenamiento\n",
    "torch_train_loader = DataLoader(Subset(torch_set, train_idx), \n",
    "                                shuffle=True, batch_size=32)\n",
    "# Data loader de validación\n",
    "torch_valid_loader = DataLoader(Subset(torch_set, valid_idx), \n",
    "                                shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrón multicapa\n",
    "\n",
    "Implementemos un perceptrón multicapa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_onehiddenlayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_hidden, n_output): \n",
    "        super(type(self), self).__init__()\n",
    "        \n",
    "        self.hidden = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.output = torch.nn.Linear(n_hidden, n_output)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.hidden(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y si queremos más de una capa oculta?\n",
    "\n",
    "- Podemos añadir explicitamente otra capa\n",
    "- Podemos usar [`torch.nn.ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) para crear capas ocultas programaticamente. `nn.ModuleList` funciona como una lista de capas que luego podemos iterar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, neurons=[2, 1]): \n",
    "        super(type(self), self).__init__()\n",
    "        \n",
    "        assert len(neurons) >= 2, \"Se necesita al menos capa de entrada y capa de salida\"\n",
    "        self.hidden = torch.nn.ModuleList()\n",
    "        for k in range(len(neurons)-2):\n",
    "            self.hidden.append(torch.nn.Linear(neurons[k], neurons[k+1]))                \n",
    "        \n",
    "        self.output = torch.nn.Linear(neurons[-2], neurons[-1])\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ModuleList es un objeto iterable\n",
    "        for k, layer in enumerate(self.hidden):\n",
    "            x = self.activation(layer(x))\n",
    "\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos algunas funciones utilitarias\n",
    "\n",
    "Detalles\n",
    "\n",
    "- Algunas capas de PyTorch tales como *Dropout* y *Batch Normalization*, tienen comportamiento distinto en entrenamiento y evaluación\n",
    "- Podemos cambiar el \"modo\" de nuestra red neuronal llamando a las funciones internas `train()` y `eval()` respectivamente \n",
    "- Todo modelo que herede de `nn.Module` tendrá definidas dichas funciones\n",
    "- El contexto `torch.no_grad()` evita que se construya el grafo, lo cual aumenta la velocidad de la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(batch): \n",
    "    model.train() \n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch\n",
    "    yhat = model.forward(x)\n",
    "    loss = criterion(yhat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_one_step(batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        x, y = batch\n",
    "        yhat = model.forward(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        return yhat.argmax(dim=1), y, loss.item()\n",
    "\n",
    "def update_plot(epoch):\n",
    "    XY = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))\n",
    "    Z = torch.nn.Softmax(dim=1)(model.forward(XY)).detach().numpy()[:, 0].reshape(xx.shape)\n",
    "    [ax_.cla() for ax_ in ax]\n",
    "    ax[0].contourf(xx, yy, Z, cmap=plt.cm.RdBu_r, alpha=1., vmin=0, vmax=1)\n",
    "    for i, marker in enumerate(['o', 'x', 'd']):\n",
    "        ax[0].scatter(data[labels==i, 0], data[labels==i, 1], color='k', s=10, marker=marker, alpha=0.5)\n",
    "    for i, name in enumerate(['Train', 'Validation']):\n",
    "        ax[1].plot(np.arange(0, epoch+1, step=1), running_loss[:epoch+1, i], '-', label=name+\" cost\")\n",
    "    plt.legend(); ax[1].grid()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento usando Pytorch \n",
    "\n",
    "Estudiemos\n",
    "\n",
    "- ¿Cómo cambia el resultado según la cantidad de capas y neuronas ocultas?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345) # Inicialización\n",
    "\n",
    "neurons = [n_input, 2, n_classes] # Arquitectura\n",
    "model = MLP(neurons)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "max_epochs = 100    \n",
    "running_loss = np.zeros(shape=(max_epochs, 2))\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3.5), tight_layout=True)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Loop de entrenamiento\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    for batch in torch_train_loader:\n",
    "        train_loss += train_one_step(batch)\n",
    "    running_loss[epoch, 0] = train_loss/torch_train_loader.dataset.__len__()    \n",
    "    # Loop de validación\n",
    "    for batch in torch_valid_loader:\n",
    "        valid_loss += evaluate_one_step(batch)[-1]\n",
    "    running_loss[epoch, 1] = valid_loss/torch_valid_loader.dataset.__len__()    \n",
    "    # Checkpointing  \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save({'current_epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'current_valid_loss': valid_loss\n",
    "                   }, 'best_model.pt')\n",
    "    # Actualizar gráficos\n",
    "    update_plot(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspeccionando la solución\n",
    "\n",
    "- Cada neurona es un hiperplano\n",
    "- La primera capa son hiperplanos en el espacio de los datos\n",
    "- La segunda capa es un hiperplano en la salida de la primera capa\n",
    "- La segunda capa no es un hiperplano en el espacio de los datos, sino una combinación no-lineal de hiperplanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert neurons[1] == 2, \"Esta gráfica no funciona con más de 2 neuronas en capa oculta\"\n",
    "\n",
    "model = MLP(neurons)\n",
    "model.load_state_dict(torch.load('best_model.pt')['model_state_dict'])\n",
    "\n",
    "XY = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))\n",
    "Z = model.activation(model.hidden[0](XY)).detach().numpy()\n",
    "fig, ax = plt.subplots(1+n_classes-1, 2, figsize=(8, 3*(n_classes)), tight_layout=True)\n",
    "for k in range(2):\n",
    "    ax[0, k].set_title(f\"Salida neurona {k+1}\")\n",
    "    cf = ax[0, k].contourf(xx, yy, Z[:, k].reshape(xx.shape), \n",
    "                   cmap=plt.cm.BrBG_r, alpha=1., vmin=0, vmax=1)\n",
    "    fig.colorbar(cf, ax=ax[0, k])\n",
    "    for i, marker in enumerate(['o', 'x', 'd']):\n",
    "        ax[0, k].scatter(data[labels==i, 0], data[labels==i, 1], \n",
    "                         color='k', s=10, marker=marker, alpha=0.5)\n",
    "\n",
    "for k in range(n_classes-1):        \n",
    "    Z = torch.nn.Softmax(dim=1)(model.forward(XY))[:,k].detach().numpy()\n",
    "    ax[k+1, 1].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu_r, alpha=1.)\n",
    "    for i, marker in enumerate(symbols[:n_classes]):\n",
    "        ax[k+1, 1].scatter(data[labels==i, 0], data[labels==i, 1], color='k', s=10, marker=marker, alpha=0.5)\n",
    "\n",
    "    Z = model.activation(model.output(XY))[:,k].detach().numpy()\n",
    "    ax[k+1, 0].contourf(xx, yy, Z.reshape(xx.shape), cmap=plt.cm.RdBu_r, alpha=1.)\n",
    "    ax[k+1, 0].set_xlim([0, 1]); ax[k+1, 0].set_ylim([0, 1]);\n",
    "    ax[k+1, 0].set_xlabel('Salida Neurona 1'); \n",
    "    ax[k+1, 0].set_ylabel('Salida neurona 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento usando Ignite\n",
    "\n",
    "Ignite es una librería de alto nivel \n",
    "\n",
    "Provee engines, eventos, manejadores y métricas\n",
    "\n",
    "- Los engines se encargan de entrenar y evaluar la red. Se ponen en marcha usando el atributo `run`\n",
    "- Una métrica es un valor con el que evaluamos nuestra red (Loss, accuracy, f1-score)\n",
    "- Los manejadores nos permiten realizar acciones cuando se cumple un evento, por ejemplo\n",
    "    - Imprimir los resultados\n",
    "    - Guardar el mejor modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Loss, Accuracy\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "\n",
    "\n",
    "torch.manual_seed(12345) # Inicialización\n",
    "neurons = [2, 2, n_classes]\n",
    "model = MLP(neurons)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "max_epochs = 100  \n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion) # Creo un engine para entrenar\n",
    "metrics = {'Loss': Loss(criterion), 'Acc': Accuracy()}\n",
    "evaluator = create_supervised_evaluator(model, metrics=metrics) # Creo un engine para validar\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=10)) # Cada 10 epocas\n",
    "def log_results(engine):\n",
    "    evaluator.run(torch_valid_loader) # Evaluo el conjunto de validación\n",
    "    loss = evaluator.state.metrics['Loss']\n",
    "    acc = evaluator.state.metrics['Acc']\n",
    "    print(f\"Epoca: {engine.state.epoch} \\t Loss: {loss:.2f} \\t Accuracy: {acc:.2f}\")\n",
    "    \n",
    "best_model_handler = ModelCheckpoint(dirname='.', \n",
    "                                     require_empty=False, \n",
    "                                     filename_prefix=\"best\", \n",
    "                                     n_saved=1,\n",
    "                                     score_function=lambda engine: -engine.state.metrics['Loss'],\n",
    "                                     score_name=\"val_loss\")\n",
    "\n",
    "# Lo siguiente se ejecuta cada ves que termine el loop de validación\n",
    "evaluator.add_event_handler(Events.COMPLETED,\n",
    "                            best_model_handler, {'mymodel': model})\n",
    "\n",
    "trainer.run(torch_train_loader, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(neurons)\n",
    "#model.load_state_dict(torch.load('best_mymodel_val_loss=-33.8630.pt'))\n",
    "\n",
    "fig, ax = plt.subplots(1, n_classes, figsize=(3*n_classes, 3), tight_layout=True)\n",
    "XY = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()].astype('float32'))\n",
    "Z = torch.nn.Softmax(dim=1)(model.forward(XY)).detach().numpy()\n",
    "for j in range(n_classes):\n",
    "    ax[j].contourf(xx, yy, Z[:, j].reshape(xx.shape), cmap=plt.cm.RdBu_r, alpha=1.)\n",
    "    for i, marker in enumerate(symbols[:n_classes]):\n",
    "        ax[j].scatter(data[labels==i, 0], data[labels==i, 1], \n",
    "                      color='k', s=10, marker=marker, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si los *engine* por defecto no cumplen con lo que necesitamos, podemos crear un engine en base a una función como sigue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "\n",
    "# Esto es lo que hace el engine de entrenamiento\n",
    "def train_one_step(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch\n",
    "    yhat = model.forward(x)\n",
    "    loss = criterion(yhat, y.unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item() # Este output puede llamar luego como trainer.state.output\n",
    "\n",
    "# Esto es lo que hace el engine de evaluación\n",
    "def evaluate_one_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        x, y = batch\n",
    "        yhat = model.forward(x)\n",
    "        return yhat, y\n",
    "\n",
    "trainer = Engine(train_one_step)\n",
    "evaluator = Engine(evaluate_one_step)\n",
    "for name, metric in metrics.items():\n",
    "    metric.attach(evaluator, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch, Ignite y Tensorboard\n",
    "\n",
    "Podemos usar la herramienta [tensorboard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) para visualizar el entrenamiento de la red en vivo y/o comparar distintos entrenamientos\n",
    "\n",
    "- Instalar tensorboard versión 1.15 o mayor con conda\n",
    "\n",
    "- Escribir en un terminal\n",
    "\n",
    "        tensorboard --logdir=/tmp/tensorboard/\n",
    "\n",
    "- Apuntar el navegador a \n",
    "\n",
    "        https://localhost:6006 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "torch.manual_seed(12345) # Inicialización\n",
    "neurons = [2, 2, n_classes]\n",
    "model = MLP(neurons)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "max_epochs = 100  \n",
    "\n",
    "# Creación de engines y asignación de métricas\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "metrics = {'Loss': Loss(criterion), 'Acc': Accuracy()}\n",
    "evaluator = create_supervised_evaluator(model, metrics=metrics) \n",
    "\n",
    "# Contexto de escritura de datos para tensorboard\n",
    "with SummaryWriter(log_dir=f'/tmp/tensorboard/experimento_interesante_{time.time_ns()}') as writer:\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED(every=1)) # Cada 1 epocas\n",
    "    def log_results(engine):\n",
    "        evaluator.run(torch_train_loader) # Evaluo el conjunto de entrenamiento\n",
    "        writer.add_scalar(\"train/loss\", evaluator.state.metrics['Loss'], engine.state.epoch)\n",
    "        writer.add_scalar(\"train/accy\", evaluator.state.metrics['Acc'], engine.state.epoch)\n",
    "        \n",
    "        evaluator.run(torch_valid_loader) # Evaluo el conjunto de validación\n",
    "        writer.add_scalar(\"valid/loss\", evaluator.state.metrics['Loss'], engine.state.epoch)\n",
    "        writer.add_scalar(\"valid/accy\", evaluator.state.metrics['Acc'], engine.state.epoch)\n",
    "\n",
    "    best_model_handler = ModelCheckpoint(dirname='.', require_empty=False, filename_prefix=\"best\", n_saved=1,\n",
    "                                         score_function=lambda engine: -engine.state.metrics['Loss'],\n",
    "                                         score_name=\"val_loss\")\n",
    "\n",
    "    # Lo siguiente se ejecuta cada ves que termine el loop de validación\n",
    "    evaluator.add_event_handler(Events.COMPLETED, \n",
    "                                best_model_handler, {'mymodel': model})\n",
    "\n",
    "    trainer.run(torch_train_loader, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

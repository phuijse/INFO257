{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando un modelo pre-entrenado\n",
    "\n",
    "[`torchvision.models`](https://pytorch.org/docs/stable/torchvision/models.html) ofrece una serie de modelos famosos de la literatura de *deep learning*\n",
    "\n",
    "Por defecto el modelo se carga con pesos aleatorios\n",
    "\n",
    "Si indicamos `pretrained=True` se descarga un modelo entrenado\n",
    "\n",
    "Se pueden escoger modelos para clasificar, localizar y segmentar\n",
    "\n",
    "## Modelo para clasificar imágenes\n",
    "\n",
    "torchvision tiene una basta cantidad de modelos para clasificar incluyendo distintas versiones de VGG, ResNet, AlexNet, GoogLeNet, DenseNet, entre otros\n",
    "\n",
    "Cargaremos un modelo [resnet18](https://arxiv.org/pdf/1512.03385.pdf) [pre-entrenado](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.resnet18) en [ImageNet](http://image-net.org/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=True, progress=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos pre-entrenados esperan imágenes con\n",
    "- tres canales (RGB)\n",
    "- al menos 224x224 píxeles\n",
    "- píxeles entre 0 y 1 (float)\n",
    "- normalizadas con \n",
    "\n",
    "        normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "img = Image.open(\"img/dog.jpg\")\n",
    "\n",
    "my_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                   transforms.CenterCrop(224),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                                        std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "# Las clases con probabilidad más alta son\n",
    "probs = torch.nn.Softmax(dim=1)(model.forward(my_transform(img).unsqueeze(0)))\n",
    "\n",
    "best = probs.argsort(descending=True)\n",
    "display(best[0, :5], probs[0, best[0, :5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿A qué corresponde estas clases?\n",
    "\n",
    "Clases de ImageNet: https://gist.github.com/ageitgey/4e1342c10a71981d0b491e1b8227328b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo para detectar entidades en imágenes\n",
    "\n",
    "Adicional a los modelos de clasificación torchvision también tiene modelos para\n",
    "- Detectar entidades en una imagen: Faster RCNN\n",
    "- Hacer segmentación por instancia: Mask RCNN\n",
    "- Hacer segmentación semántica: FCC, DeepLab\n",
    "- Clasificación de video \n",
    "\n",
    "A continuación probaremos la [Faster RCNN](https://arxiv.org/abs/1506.01497) para hace detección\n",
    "\n",
    "Este modelo fue pre-entrenado en la base de datos [COCO](https://cocodataset.org/)\n",
    "\n",
    "El modelo retorna un diccionario con\n",
    "- 'boxes': Los bounding box de las entidades\n",
    "- 'labels': La etiqueta de la clase más probable de la entidad\n",
    "- 'score': La probabilidad de la etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "img = Image.open(\"img/pelea.jpg\") # No require normalización de color\n",
    "img_tensor = transform(img)\n",
    "\n",
    "result = model(img_tensor.unsqueeze(0))[0]\n",
    "\n",
    "def filter_results(result, threshold=0.9):\n",
    "    mask = result['scores'] > 0.9\n",
    "    bbox = result['boxes'][mask].detach().cpu().numpy()\n",
    "    lbls = result['labels'][mask].detach().cpu().numpy()\n",
    "    return bbox, lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont, ImageDraw\n",
    "fnt = ImageFont.truetype(\"arial.ttf\", 20) \n",
    "\n",
    "label2name = {1: 'persona', 2: 'bicicleta', 3: 'auto', 4: 'moto', \n",
    "              8: 'camioneta', 18: 'perro'}\n",
    "\n",
    "def draw_rectangles(img, bbox, lbls):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for k in range(len(bbox)):\n",
    "        if lbls[k] in label2name.keys():\n",
    "            draw.rectangle(bbox[k], fill=None, outline='white', width=2)\n",
    "            draw.text([int(d) for d in bbox[k][:2]], label2name[lbls[k]], font=fnt, fill='white')\n",
    "\n",
    "bbox, lbls = filter_results(result)\n",
    "img = Image.open(\"img/pelea.jpg\")\n",
    "draw_rectangles(img, bbox, lbls)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferencia de Aprendizaje\n",
    "\n",
    "\n",
    "A continuación usaremos la técnicas de transferencia de aprendizaje para aprender un clasificador de imágenes para un fragmento de la base de datos food 5k\n",
    "\n",
    "El objetivo es clasificar si la imagen corresponde a comida o no\n",
    "\n",
    "Guardamos las imagenes con la siguiente estructura de carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls img/food5k/\n",
    "!ls img/food5k/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto podemos usar `torchvision.datasets.ImageFolder` para crear los dataset de forma muy sencilla\n",
    "\n",
    "Dado que usaremos un modelo preentrenado debemos transformar entregar las imágenes en tamaño 224x224 y con color normalizado\n",
    "\n",
    "Usaremos también aumentación de datos en el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('img/food5k/train', transform=train_transforms)\n",
    "valid_dataset = datasets.ImageFolder('img/food5k/valid', transform=valid_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    break\n",
    "    \n",
    "fig, ax = plt.subplots(1, 6, figsize=(9, 2), tight_layout=True)\n",
    "for i in range(6):\n",
    "    ax[i].imshow(image[i].permute(1,2,0).numpy())\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(label[i].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el modelo ResNet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True, progress=True)\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso re-entrenaremos sólo la última capa: `fc`\n",
    "\n",
    "Las demás capas las congelaremos\n",
    "\n",
    "Para congelar una capa simplemente usamos `requires_grad=False` en sus parámetros\n",
    "\n",
    "Cuando llamemos `backward` no se calculará gradiente para estas capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Congelamos todos los parámetros\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "\n",
    "# Recuperamos el número de neuronas de la última capa\n",
    "neurons = model.fc.in_features \n",
    "# La reemplazamos por una nueva capa de salida\n",
    "model.fc = torch.nn.Linear(neurons, 2) \n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model.forward(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in valid_loader:\n",
    "        yhat = model.forward(x)\n",
    "        loss = criterion(yhat, y)\n",
    "        epoch_loss += loss.item()\n",
    "    print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, predictions = [], []\n",
    "for mbdata, label in valid_loader:\n",
    "    logits = model.forward(mbdata)\n",
    "    predictions.append(logits.argmax(dim=1).detach().numpy())\n",
    "    targets.append(label.numpy())\n",
    "predictions = np.concatenate(predictions)\n",
    "targets = np.concatenate(targets)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(targets, predictions)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(targets, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen\n",
    "\n",
    "Aspectos a considerar durante el entrenamiento de redes neuronales\n",
    "- Arquitecturas: cantidad y organización de capas, funciones de activación\n",
    "- Funciones de costo, optimizadores y sus parámetros (tasa de aprendizaje, momentum)\n",
    "- Verificar convergencia y sobreajuste:\n",
    "    - Checkpoint: Guardar el último modelo y el con menor costo de validación\n",
    "    - Early stopping: Detener el entrenamiento si el error de validación no disminuye en un cierto número de épocas\n",
    "- Inicialización de los parámetros: Probar varios entrenamientos desde inicios aleatorios distintos\n",
    "- Si el modelo se sobreajusta pronto\n",
    "    - Disminuir complejidad\n",
    "    - Regularizar: Aumentación de datos, decaimiento de pesos, Dropout\n",
    "- Si quiero aprovechar un modelo preentrenado\n",
    "    - Transferencia de aprendizaje\n",
    "    - [Zoológico de modelos](https://modelzoo.co/)\n",
    "\n",
    "Estrategia agil\n",
    "> Desarrolla rápido e itera: Empieza simple. Propón una solución, impleméntala, entrena y evalua. Analiza las fallas, modifica e intenta de nuevo\n",
    "\n",
    "Mucho exito en sus desarrollos futuros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

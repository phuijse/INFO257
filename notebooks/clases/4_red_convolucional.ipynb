{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import animation\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Convolucionales\n",
    "\n",
    "[Slides 49-88](https://docs.google.com/presentation/d/1IJ2n8X4w8pvzNLmpJB-ms6-GDHWthfsJTFuyUqHfXg8/edit#slide=id.g3a1a71fe7e_8_192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Convolucional en PyTorch\n",
    "\n",
    "Las redes neuronales convolucionales utilizan principalmente tres tipos de capas\n",
    "\n",
    "## [Capas convolucionales](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "\n",
    "- Las neuronas de estas capas se organizan en filtros \n",
    "- Se realiza la correlación cruzada entre la imagen de entrada y los filtros\n",
    "- Existen capas convolucionales 1D, 2D y 3D\n",
    "\n",
    "\n",
    "[Visualización de convoluciones con distintos tamaños, strides, paddings, dilations](https://github.com/vdumoulin/conv_arithmetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los argumentos de la capa convolución de dos dimensiones son:\n",
    "\n",
    "```python\n",
    "torch.nn.Conv2d(in_channels, #Cantidad de canales de la imagen de entrada\n",
    "                out_channels, #Cantidad de bancos de filtro\n",
    "                kernel_size, #Tamaño de los filtros (entero o tupla)\n",
    "                stride=1, #Paso de los filtros\n",
    "                padding=0, #Cantidad de filas y columnas para agregar a la entrada antes de filtrar\n",
    "                dilation=1, #Espacio entre los pixeles de los filtros\n",
    "                groups=1, #Configuración cruzada entre filtros de entrada y salida\n",
    "                bias=True,  #Utilizar sesgo (b)\n",
    "                padding_mode='zeros' #Especifica como agregar nuevas filas/columnas (ver padding)\n",
    "                )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Capas de pooling](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
    "\n",
    "- Capa que reduce la dimensión (tamaño) de su entrada\n",
    "- Se usa tipicamente luego de una capa de convolución \"activada\"\n",
    "- Realiza una operación no entrenable: \n",
    "    - Promedio de los píxeles en una región (kernel_size=2, stride=2)\n",
    "    \n",
    "            1 2 1 0\n",
    "            2 3 1 2      2.00 1.00\n",
    "            0 1 0 1      0.75 0.25\n",
    "            2 0 0 0\n",
    "            \n",
    "    - Máximo de los pixeles en una región (kernel_size=2, stride=2)\n",
    "   \n",
    "            1 2 1 0\n",
    "            2 3 1 2      3 2\n",
    "            0 1 0 1      2 1\n",
    "            2 0 0 0\n",
    "- Estas capas ayudan a reducir la complejidad del modelo\n",
    "- También otorgan \"invarianza local a la traslación\", es decir que la posición donde estaba el patrón es menos relevante luego de aplicar pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los argumentos de MaxPooling para entradas de dos dimensiones son:\n",
    "\n",
    "```python\n",
    "torch.nn.MaxPool2d(kernel_size, # Mismo significado que en Conv2d\n",
    "                   stride=None, # Mismo significado que en Conv2d\n",
    "                   padding=0, #Mismo significado que en Conv2d\n",
    "                   dilation=1, #Mismo significado que en Conv2d\n",
    "                   return_indices=False, #Solo necesario para hacer unpooling\n",
    "                   ceil_mode=False #Usar ceil en lugar de floor para calcular el tamaño de la salida\n",
    "                  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Capas completamente conectadas](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)\n",
    "\n",
    "- Idénticas a las usadas en redes tipo MLP\n",
    "- Realizan la operación: $Z = WX + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los argumentos son:\n",
    "\n",
    "```python\n",
    "torch.nn.Linear(in_features, #Neuronas en la entrada\n",
    "                out_features,  #Neuronas en la salida\n",
    "                bias=True  #Utilizar sesgo (b)\n",
    "                )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Torchvision](https://pytorch.org/docs/stable/torchvision/index.html)\n",
    "\n",
    "Es una librería utilitaria de PyTorch que facilita considerablemente el trabajo con imágenes\n",
    "\n",
    "- Funcionalidad para descargar sets de benchmark: MNIST, CIFAR, IMAGENET, ...\n",
    "- Modelos clásicos pre-entrenados: Lenet5, AlexNet\n",
    "- Funciones para importar imágenes en distintos formatos\n",
    "- Funciones de transformación para hacer aumentación de datos en imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Base de datos de imágenes de dígitos manuscritos MNIST\n",
    "\n",
    "- Imágenes de 28x28 píxeles en escala de grises\n",
    "- Diez categorías: Dígitos manuscritos del cero al nueve\n",
    "- 60.000 imágenes de entrenamiento, 10.000 imágenes de prueba\n",
    "- Por defecto las imágenes vienen en [formato PIL](https://pillow.readthedocs.io/en/stable/) (entero 8bit), usamos la transformación [`ToTensor()`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor) para convertirla a tensor en float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"800\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "mnist_train_data = torchvision.datasets.MNIST(root='~/datasets/',\n",
    "                                              train=True, download=True, \n",
    "                                              transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "mnist_test_data = torchvision.datasets.MNIST(root='~/datasets/',\n",
    "                                             train=False, download=True, \n",
    "                                             transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "image, label = mnist_train_data[0]\n",
    "display(len(mnist_train_data), type(image), image.dtype, type(label))\n",
    "fig, ax = plt.subplots(1, 10, figsize=(8, 1.5), tight_layout=True)\n",
    "idx = np.random.permutation(len(mnist_train_data))[:10]\n",
    "for k in range(10):\n",
    "    image, label = mnist_train_data[idx[k]]\n",
    "    ax[k].imshow(image[0, :, :].numpy(), cmap=plt.cm.Greys_r)\n",
    "    ax[k].axis('off');\n",
    "    ax[k].set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "Creamos dataloaders de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Set de entrenamiento y validación estratíficados\n",
    "sss = sklearn.model_selection.StratifiedShuffleSplit(train_size=0.75).split(mnist_train_data.data, \n",
    "                                                                            mnist_train_data.targets)\n",
    "train_idx, valid_idx = next(sss)\n",
    "\n",
    "# Data loader de entrenamiento\n",
    "train_dataset = Subset(mnist_train_data, train_idx)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "\n",
    "# Data loader de validación\n",
    "valid_dataset = Subset(mnist_train_data, valid_idx)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mi primera red convolucional para clasificar en pytorch\n",
    "\n",
    "Clasificaremos la base de datos MNIST \n",
    "\n",
    "Para esto implementaremos la clásica arquitectura Lenet5\n",
    "\n",
    "<img src=\"img/LeNet5.png\" width=\"800\">\n",
    "\n",
    "La arquitectura considera\n",
    "- Dos capas convolucionales con 8 y 16 bancos de filtros, respectivamente\n",
    "- Las capas convolucionales usan filtros de 5x5 píxeles\n",
    "- Se usa max-pooling de tamaño 2x2 y stride 2\n",
    "- La primera capa convolucional espera un minibatch de imágenes de 1 canal (blanco y negro)\n",
    "- Usaremos la función de activación [Rectified Linear Unit (ReLU)](https://pytorch.org/docs/stable/nn.html#relu)\n",
    "- Se usan tres capas completamente conectadas con 120, 84 y 10 neuronas, respectivamente\n",
    "\n",
    "> Podemos usar `reshape` o `view` para convertir un tensor de 4 dimensiones a dos dimensiones.  Esto prepara un tensor que sale de una capa convolucional (o pooling) para ingresarlo a las capas completamente conectadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lenet5()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Lenet5(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=20, n_filters=8):\n",
    "        super(type(self), self).__init__()\n",
    "        # Completar\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Completar\n",
    "        pass\n",
    "    \n",
    "model = Lenet5()\n",
    "display(model)\n",
    "\n",
    "model.forward(mnist_train_data[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación multiclase en PyTorch\n",
    "\n",
    "Para hacer clasificación con **más de dos categorías** usamos la [entropía cruzada](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
    "\n",
    "```python\n",
    "    torch.nn.CrossEntropyLoss()\n",
    "```\n",
    "\n",
    "- Si el problema de clasificación es de $M$ categorías la última capa de la red debe tener $M$ neuronas\n",
    "- Adicionalmente no se debe usar función de activación ya que `CrossEntropyLoss` la aplica de forma interna\n",
    "\n",
    "Para evaluar la red debemos aplicar de forma manual \n",
    "- `torch.nn.Softmax(dim=1)`\n",
    "- `torch.nn.LogSoftmax(dim=1)`\n",
    "\n",
    "a la salida de la red\n",
    "\n",
    "> Luego podemos usar el atributo `argmax(dim=1)` para encontrar la clase más probable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente con paso adaptivo\n",
    "\n",
    "Para acelerar el entrenamiento podemos usar un algoritmo de [gradiente descendente con paso adaptivo](https://arxiv.org/abs/1609.04747)\n",
    "\n",
    "Un ejemplo ampliamente usado es [Adam](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "- Se utiliza la historia de los gradientes\n",
    "- Se utiliza momentum (inercia)\n",
    "- Cada parámetro tiene un paso distinto\n",
    "\n",
    "```python\n",
    "    torch.optim.Adam(params,  **Parámetros de la red neuronal**\n",
    "                     lr=0.001,  **Tasa de aprendizaje inicial**\n",
    "                     betas=(0.9, 0.999),  **Factores de olvido de los gradientes históricos**\n",
    "                     eps=1e-08, **Término para evitar división por cero**\n",
    "                     weight_decay=0, **Regulariza los pesos de la red si es mayor que cero**\n",
    "                     amsgrad=False **Corrección para mejorar la convergencia de Adam en ciertos casos**\n",
    "                     )\n",
    "```\n",
    "\n",
    "**Atención**\n",
    "\n",
    "Esta es un área de investigación activa. [Papers recientes indican que Adam llega a un óptimo más rápido que SGD, pero ese óptimo podría no ser mejor que el obtenido por SGD](https://arxiv.org/abs/1712.07628)\n",
    "\n",
    "> Siempre prueba tus redes con distintos optimizadores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de la red convolucional\n",
    "\n",
    "- Si tenemos acceso a una GPU podemos usar el atributo `.cuda()` o `.to()` para enviar el modelo y los datos a la GPU para acelerar los cálculos\n",
    "- Actualizamos los parámetros en el conjunto de entrenamiento\n",
    "- Medimos la convergencia en el conjunto de validación\n",
    "- Guardamos el modelo con mejor eror de validación\n",
    "- Usaremos [tqdm](https://tqdm.github.io/) para medir el tiempo por iteración dentro de jupyter. Instalar con [conda](https://anaconda.org/conda-forge/tqdm) o [pip](https://pypi.org/project/tqdm/)\n",
    "\n",
    "\n",
    "### Visualización de entrenamiento usando tensorboard\n",
    "\n",
    "Podemos usar la herramienta [tensorboard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) para visualizar el entrenamiento de la red en vivo y/o comparar distintos entrenamientos\n",
    "\n",
    "- Instalar tensorboard versión 1.15 o mayor con [conda](https://anaconda.org/conda-forge/tensorboard) o pip\n",
    "\n",
    "- Escribir en un terminal\n",
    "\n",
    "        tensorboard --logdir=/tmp/tensorboard/\n",
    "\n",
    "- Apuntar el navegador a \n",
    "\n",
    "        https://localhost:6006 \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "use_gpu = False\n",
    "nepochs = 100\n",
    "best_valid = np.inf\n",
    "writer = SummaryWriter(log_dir=\"/tmp/tensorboard/red_convolucional_simple/\"+str(time.time_ns()),\n",
    "                       flush_secs=20)\n",
    "\n",
    "# Enviar modelo a la GPU\n",
    "if use_gpu:\n",
    "    nnet = nnet.cuda()\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
    "\n",
    "for k in tqdm_notebook(range(nepochs)): \n",
    "    # Entrenamiento\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in train_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "        # Calcular gradientes\n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        # Actualizar parámetros\n",
    "        optimizer.step()\n",
    "    # Enviar información a tensorboard\n",
    "    writer.add_scalar('Train/Loss', epoch_loss/len(train_idx), k)\n",
    "    writer.add_scalar('Train/Acc', epoch_acc/len(train_idx), k)\n",
    "    # Validación\n",
    "    epoch_loss, epoch_acc = 0.0, 0.0\n",
    "    for mbdata, mblabel in valid_loader:\n",
    "        if use_gpu:\n",
    "            mbdata, mblabel = mbdata.cuda(), mblabel.cuda()\n",
    "        # Inferencia\n",
    "        prediction = model.forward(mbdata)\n",
    "        # Estimar el error\n",
    "        loss = criterion(prediction, mblabel)  \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (torch.nn.Softmax(dim=1)(prediction).argmax(dim=1) == mblabel).sum().item()        \n",
    "    # Enviar información a tensorboard\n",
    "    writer.add_scalar('Valid/Loss', epoch_loss/len(valid_idx), k)\n",
    "    writer.add_scalar('Valid/Acc', epoch_acc/len(valid_idx), k)\n",
    "    \n",
    "    if k % 5 == 0:\n",
    "        if epoch_loss/len(valid_idx) < best_valid:\n",
    "            best_valid = epoch_loss/len(valid_idx)\n",
    "            torch.save({'epoca': k,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),                        \n",
    "                        'loss': epoch_loss/len(valid_idx)}, \n",
    "                       '/home/phuijse/models/best_model.pt') # MODIFICA LA RUTA \n",
    "\n",
    "# Retornar modelo a la CPU\n",
    "if use_gpu:\n",
    "    nnet = model.cpu()\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluando la red en el conjunto de test\n",
    "\n",
    "Primero recuperamos la mejor red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mi_red_convolucional()\n",
    "\n",
    "model.load_state_dict(torch.load('/home/phuijse/models/best_model.pt')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para clasificar un ejemplo de test debemos pasar la salida por una activación SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = mnist_test_data[np.random.randint(1000)]\n",
    "# Usamos unsqueeze para convertirlo en un minibatch de 1 elemento:\n",
    "y = torch.nn.Softmax(dim=1)(model.forward(image.unsqueeze(0)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(5, 3))\n",
    "ax[0].bar(range(10), height=y.detach().numpy()[0])\n",
    "ax[0].set_xticks(range(10))\n",
    "ax[1].set_title(\"Etiqueta: %d\" %(label))\n",
    "ax[1].imshow(image.numpy()[0, :, :], cmap=plt.cm.Greys_r);\n",
    "ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar la red en el conjunto de prueba y construir una tabla de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(mnist_test_data, shuffle=False, batch_size=512)\n",
    "test_targets = mnist_test_data.targets.numpy()\n",
    "prediction_test = []\n",
    "for mbdata, label in test_loader:\n",
    "    logits = model.forward(mbdata)\n",
    "    prediction_test.append(torch.argmax(logits, dim=1).detach().numpy())\n",
    "\n",
    "prediction_test = np.concatenate(prediction_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(test_targets, prediction_test)\n",
    "display(cm)\n",
    "\n",
    "print(classification_report(test_targets, prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una tarea muy importante es analizar los errores de la red\n",
    "\n",
    "- Buscar ejemplos que estaban mal etiquetados\n",
    "- Proponer mejoras (nuevos atributos, capas, etc)\n",
    "\n",
    "Por ejemplo si visualizamos los que deberían ser $7$ pero son clasificados como otro dígito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.where((test_targets == 9) & ~(prediction_test == 9))[0]\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(8, 2), tight_layout=True)\n",
    "for i in range(10):\n",
    "    ax[i].imshow(mnist_test_data[idx[i]][0].numpy()[0, :, :], cmap=plt.cm.Greys_r)\n",
    "    ax[i].set_title(prediction_test[idx[i]])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando los filtros aprendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 8, figsize=(7, 2), tight_layout=True)\n",
    "w = model.conv1.weight.data.numpy()\n",
    "\n",
    "for i in range(8):    \n",
    "    ax[i].imshow(w[i, 0, :, :])\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aumentación de datos\n",
    "\n",
    "Si tenemos un dataset de imágenes muy pequeño y la red que estamos entrenando se está sobreajustando podemos intentar incrementarlo usando **transformaciones**\n",
    "\n",
    "Si rotamos, trasladamos o cambiamos el brillo de una imagen obtendremos una nueva imagen \"casi siempre\" de la misma clase\n",
    "\n",
    "*torchvision* tiene funciones implementadas para hacer transformaciones:\n",
    "\n",
    "- [Rotación aleatoria](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomRotation)\n",
    "- [Espejamiento aleatorio](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip)\n",
    "- [Cropping aleatorio](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomCrop): Recortar la imagen\n",
    "- [Cambios aleatorios de brillo y contraste](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ColorJitter)\n",
    "- [Transformación afin aleatoria](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "- [entre otros](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "Cada transformación permite especificar límites, por ejemplo \"máximo ángulo de rotación\", \"máxima distorsión de brillo\", etc\n",
    "\n",
    "Las transformaciones también sirven para hacer que la red gane \"invarianzas\"\n",
    "\n",
    "**Ejemplo:** Si entrenamos con copias rotadas de nuestras imágenes, la red se volverá invariante a la rotación\n",
    "\n",
    "### ATENCIÓN\n",
    "\n",
    "> Las transformaciones que apliquemos no deben cambiar la interpretación de clase\n",
    "\n",
    "- Si rotas un seis en 180 grados se convierte en un nueve\n",
    "- Si cambias demasiado el tono (hue) podrías obtener colores distintos a la realidad (perro verde?)\n",
    "\n",
    "### Transformaciones aleatorias con [torchvision](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "\n",
    "La mayoría de las transformaciones están diseñadas para aplicarse sobre imágenes en formato PIL\n",
    "\n",
    "Podemos componer varias transformaciones usando [`torchvision.transforms.Compose`](https://pytorch.org/docs/stable/torchvision/transforms.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url, filename = (\"https://i.kym-cdn.com/photos/images/newsfeed/000/674/934/422.jpg\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"dog.jpg\")\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize(200),\n",
    "                                            torchvision.transforms.RandomHorizontalFlip(),\n",
    "                                            torchvision.transforms.RandomRotation(degrees=30),\n",
    "                                            torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, \n",
    "                                                                               saturation=0.5, hue=0.0),\n",
    "                                           ])\n",
    "\n",
    "display(torchvision.transforms.Resize(200)(img))\n",
    "display(transform(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando con datos aumentados\n",
    "\n",
    "Podemos componer una transformación y añadirla a un dataset\n",
    "\n",
    "Luego cuando usamos el dataloader se generaran imágenes con transformaciones aleatorias\n",
    "\n",
    "\n",
    "\n",
    "### IMPORTANTE: SOLO SE AUMENTA EL CONJUNTO DE ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.RandomAffine(degrees=30, translate=(0.2, 0.2), \n",
    "                                                                                scale=(0.5, 1.5), shear=None, \n",
    "                                                                                resample=False, fillcolor=0),\n",
    "                                            torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, \n",
    "                                                                               saturation=0.5, hue=0.0),\n",
    "                                            torchvision.transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "mnist_train_data = torchvision.datasets.MNIST(root='/home/phuijse/datasets/',\n",
    "                                             train=True, download=True, \n",
    "                                             transform=transform)\n",
    "\n",
    "train_loader = DataLoader(mnist_train_data, shuffle=False, batch_size=32)\n",
    "\n",
    "for image, label in train_loader:\n",
    "    break\n",
    "\n",
    "fig, ax = plt.subplots(4, 8, figsize=(7, 4), tight_layout=True)\n",
    "for k in range(32):\n",
    "    i, j = np.unravel_index(k, (4, 8))\n",
    "    ax[i, j].axis('off')\n",
    "    ax[i, j].set_title(label[k].numpy())\n",
    "    ax[i, j].imshow(image[k].numpy()[0, :, :], cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando un modelo pre-entrenado\n",
    "\n",
    "[`torchvision.models`](https://pytorch.org/docs/stable/torchvision/models.html) ofrece una serie de modelos famosos de la literatura de *deep learning*\n",
    "\n",
    "Por defecto el modelo se carga con pesos aleatorios\n",
    "\n",
    "Se pueden escoger modelos para clasificar, localizar y segmentar\n",
    "\n",
    "Si indicamos `pretrained=True` se descarga un modelo entrenado\n",
    "\n",
    "Por ejemplo, podemos cargar [resnet18](https://arxiv.org/pdf/1512.03385.pdf) [pre-entrenado](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.resnet18) en [ImageNet](http://image-net.org/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_model = torchvision.models.resnet18(pretrained=True, progress=True)\n",
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos pre-entrenados esperan imágenes con\n",
    "- tres canales (RGB)\n",
    "- al menos 224x224 píxeles\n",
    "- píxeles entre 0 y 1 (float)\n",
    "- normalizadas con \n",
    "\n",
    "        normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/dog.jpg\", \"dog.jpg\")\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "img = Image.open(\"dog.jpg\")\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize(256),\n",
    "                                            torchvision.transforms.CenterCrop(224),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                                                                             std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "# La clase con probabilidad más alta\n",
    "resnet_model.forward(transform(img).unsqueeze(0)).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿A qué corresponde esta clase?\n",
    "\n",
    "Clases de ImageNet: https://gist.github.com/ageitgey/4e1342c10a71981d0b491e1b8227328b\n",
    "\n",
    "¿y la imagen que era?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen\n",
    "\n",
    "Aspectos a considerar durante el entrenamiento de redes neuronales\n",
    "- Arquitecturas (cantidad y organización de capas, funciones de activación)\n",
    "- Funciones de costo y Optimizadores (tasa de aprendizaje)\n",
    "- Verificar convergencia y sobreajuste:\n",
    "    - Siempre ir guardando el mejor modelo en validación (checkpoint)\n",
    "    - Detener el entrenamiento si el error de validación no disminuye \n",
    "- Inicialización de los parámetros: Probar varios entrenamientos desde inicios aleatorios distintos\n",
    "- Normalización de entrada y normalización capa a capa ([batch-normalization](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d))\n",
    "- Si el modelo se sobreajusta pronto\n",
    "    - Disminuir complejidad\n",
    "    - Regularizar (Aumentación de datos, penalización de los pesos, Dropout)\n",
    "- Si quiero aprovechar un modelo preentrenado\n",
    "    - Transferencia de aprendizaje\n",
    "    - [Zoológico de modelos](https://modelzoo.co/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferencia de Aprendizaje\n",
    "\n",
    "Ajuste fino de un modelo pre-entrenado\n",
    "\n",
    "FUTURO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localización y segmentación\n",
    "\n",
    "Encontrar y segmentar objetos en imágenes\n",
    "\n",
    "FUTURO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
